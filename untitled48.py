# -*- coding: utf-8 -*-
"""Untitled48.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LmcUU0pb737jwxEMFat0Ewz3bRZ-7LH9
"""

import tensorflow as tf
print("GPU Available:", tf.config.list_physical_devices('GPU'))

import os
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set your dataset path (using a raw string for Windows paths)
dataset_path = "/content/drive/MyDrive/fashion-mnist_train.csv"

dataset_path = "/content/drive/MyDrive/fashion-mnist_test.csv"


# Define image size and batch size for training
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

from google.colab import drive
drive.mount('/content/drive')

# Install tensorflow-datasets
!pip install tensorflow-datasets
import tensorflow as tf
import tensorflow_datasets as tfds # Import tensorflow_datasets and alias it as tfds
import shutil # Import the shutil module
import os # Import the os module

# Define dataset directory
dataset_dir = "/content/fashion-mnist-data"

# Download and prepare the fashion_mnist dataset
if os.path.exists(dataset_dir):
  shutil.rmtree(dataset_dir)

os.makedirs(dataset_dir)
train_dataset, test_dataset = tfds.load(
    name="fashion_mnist",
    split=["train", "test"],
    as_supervised=True,
    data_dir = dataset_dir
)

# Process the training dataset
os.makedirs(os.path.join(dataset_dir,'train/'), exist_ok=True)
# Process the training dataset
for index, (image, label) in enumerate(train_dataset):
  # Save the image
  image_path = os.path.join(dataset_dir,f'train/image_{index}.png')
  tf.keras.utils.save_img(image_path, image, data_format='channels_last')


# Process the test dataset
os.makedirs(os.path.join(dataset_dir,'test/'), exist_ok=True)

for index, (image, label) in enumerate(test_dataset):
  # Save the image
  image_path = os.path.join(dataset_dir,f'test/image_{index}.png')
  tf.keras.utils.save_img(image_path, image, data_format='channels_last')

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=True,
    validation_split=0.2
)
# Define dataset path
dataset_path = dataset_dir

# Load training data
train_data = train_datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'), #Correctly reference the training directory
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical", #The fashion-mnist dataset is a multi-class dataset not a binary one
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'), #Correctly reference the training directory
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical", #The fashion-mnist dataset is a multi-class dataset not a binary one
    subset='validation'
 )

# Install tensorflow-datasets
!pip install tensorflow-datasets

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D

# Load pre-trained ResNet50 model (without top layers)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze all layers except the last 10 (fine-tuning)
for layer in base_model.layers[:-10]:
    layer.trainable = False

# Add custom classification layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)         # Convert feature maps to a single vector
x = Dense(256, activation='relu')(x)     # Fully connected layer
x = Dropout(0.5)(x)                      # Dropout to reduce overfitting
output_layer = Dense(1, activation='sigmoid')(x)   # Binary classification output
# Define final model
model = Model(inputs=base_model.input, outputs=output_layer)

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Load test data
test_dir = os.path.join(dataset_path, "test")
test_data = val_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="binary"
)
# Evaluate model on test set
test_loss, test_acc = model.evaluate(test_data)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

import matplotlib.pyplot as plt

# Plot Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

# Plot Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

# saving the model
model_save_path = "/content/drive/MyDrive/AMI-train-test-original/fine_tuned_resnet.h5"
model.save(model_save_path)
print("Model saved to:", model_save_path)

# Install tensorflow-datasets
!pip install tensorflow-datasets
import tensorflow as tf
import tensorflow_datasets as tfds # Import tensorflow_datasets and alias it as tfds
import shutil # Import the shutil module
import os # Import the os module
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

# Define dataset directory
dataset_dir = "/content/fashion-mnist-data"

# Download and prepare the fashion_mnist dataset
if os.path.exists(dataset_dir):
  shutil.rmtree(dataset_dir)

os.makedirs(dataset_dir)
train_dataset, test_dataset = tfds.load(
    name="fashion_mnist",
    split=["train", "test"],
    as_supervised=True,
    data_dir = dataset_dir
)

# Process the training dataset
os.makedirs(os.path.join(dataset_dir,'train/'), exist_ok=True)
# Process the training dataset
for index, (image, label) in enumerate(train_dataset):
  # Save the image
  image_path = os.path.join(dataset_dir,f'train/image_{index}.png')
  tf.keras.utils.save_img(image_path, image, data_format='channels_last')


# Process the test dataset
os.makedirs(os.path.join(dataset_dir,'test/'), exist_ok=True)

for index, (image, label) in enumerate(test_dataset):
  # Save the image
  image_path = os.path.join(dataset_dir,f'test/image_{index}.png')
  tf.keras.utils.save_img(image_path, image, data_format='channels_last')

# Define image size and batch size for training
IMG_SIZE = (224, 224)
BATCH_SIZE = 32

# Data augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=True,
    validation_split=0.2
)
# Define dataset path
dataset_path = dataset_dir

# Load training data
train_data = train_datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'), #Correctly reference the training directory
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse", #Correctly reference the training directory
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    os.path.join(dataset_path, 'train'), #Correctly reference the training directory
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse", #Correctly reference the training directory
    subset='validation'
 )
# Load pre-trained ResNet50 model (without top layers)
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze all layers except the last 10 (fine-tuning)
for layer in base_model.layers[:-10]:
    layer.trainable = False

# Add custom classification layers on top
x = base_model.output
x = GlobalAveragePooling2D()(x)         # Convert feature maps to a single vector
x = Dense(256, activation='relu')(x)     # Fully connected layer
x = Dropout(0.5)(x)                      # Dropout to reduce overfitting
# Since fashion_mnist dataset has 10 classes the output_layer Dense has 10 units and softmax activation
output_layer = Dense(10, activation='softmax')(x)   # 10 Class classification output
# Define final model
model = Model(inputs=base_model.input, outputs=output_layer)

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy', # multi-class classification problem
              metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=[early_stop, lr_scheduler]
)
# Data augmentation for testing
test_datagen = ImageDataGenerator(
    rescale=1. / 255,
)

# Load test data
test_dir = os.path.join(dataset_path, "test")
test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse" # multiclass classification problem
)

# Evaluate model on test set
test_loss, test_acc = model.evaluate(test_data)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              loss='sparse_categorical_crossentropy', # multi-class classification problem
              metrics=['accuracy'])
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=1)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=15,
    callbacks=[early_stop, lr_scheduler]
)
# Data augmentation for testing
test_datagen = ImageDataGenerator(
    rescale=1. / 255,
)

# Load test data
test_dir = os.path.join(dataset_path, "test")
test_data = test_datagen.flow_from_directory(
    test_dir,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse" # multiclass classification problem
)

# Evaluate model on test set
test_loss, test_acc = model.evaluate(test_data)
print(f"Test Accuracy: {test_acc * 100:.2f}%")

import matplotlib.pyplot as plt

# Plot Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

# Plot Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()

# saving the model
model_save_path = "/content/drive/MyDrive/AMI-train-test-original/fine_tuned_resnet.h5"
model.save(model_save_path)
print("Model saved to:", model_save_path)

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=True,
    validation_split=0.2  # نسبة 20% للتحقق
)

train_data = train_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    subset='training'
)

val_data = train_datagen.flow_from_directory(
    dataset_path,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="sparse",
    subset='validation'
)

